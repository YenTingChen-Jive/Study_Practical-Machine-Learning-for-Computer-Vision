{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e3bda1",
   "metadata": {},
   "source": [
    "# **Chapter 05. Create Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095a01f",
   "metadata": {},
   "source": [
    "* 基本上, 任何能夠以 `4D tensor (batch × height × width × channel)` 的形式作為輸入的資料都可以應用於電腦視覺的訓練方式！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b001602",
   "metadata": {},
   "source": [
    "---\n",
    "##### 影像:\n",
    "* 影像解析度 與 運算／儲存／傳輸所需資源 的 trade-off, 建議 `選擇壓縮格式(例如JPEG)、較高閾值(95%+)、較低解析度(取決於任務精細程度)`\n",
    "* 通常影像會有3個channel(RGB), 但有些影像會有4個channel(RGBA), 其中A是 `alpha(透明度)`\n",
    "* 常規影像處理流程: `影像以壓縮字串的形式讀取, 轉換為 3D uint8 tensor, 再將 [0, 255] 的像素值轉換為 [0, 1] 的浮點數`\n",
    "* 現代最常用的排序是 [height, width, channel] 的順序, 稱為 channel-last 表達法, 例如 TensorFlow 就是如此 (早期則是 channel-first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d92e2a",
   "metadata": {},
   "source": [
    "##### 地理空間資料:\n",
    "* 從地圖產生的地理空間資料, 通常有可以被視為頻道的`柵格頻帶(raster band)`, 也就是具有特定特徵的像素(或更大的網格)被標註\n",
    "* 例如: 圖片中的河流所在的網格, 像素值會被設定為1; 或是有15個州的地圖, 會產生15個頻道, 每個頻道各自標出一個州所在的像素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c992e",
   "metadata": {},
   "source": [
    "#### 音訊(audio)與視訊(video):\n",
    "* 音訊是 1D 信號, 視訊則是 3D。 通常會建議採用專為他們設計的 ML 技術, 但其實也能夠利用處理影像的 ML 方法來簡單處理他們\n",
    "* 音訊方面, 可以用 time-domain 或 frequency-domain 的資料當作輸入, 或是也可以用 NLP 的方式來處理它\n",
    "* 視訊方面, 可以用 Conv3D 或是 其與RNN的結合來處理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826081f",
   "metadata": {},
   "source": [
    "---\n",
    "### 手動標記標籤\n",
    "* 可以`將各個類別都建立資料夾`, 再把對應的圖片放到資料夾中; 或是利用單一`CSV檔記錄下所有資料的 URL + label`\n",
    "* 但 當圖片有不只一項 label 時, 就只能使用後者的方法了\n",
    "* 大規模資料需要標記時, 可以使用 Computer Vision Annotation Tool 這個軟體來達成, 他提供了一個很好用於標記的UI介面\n",
    "* 或是當一張圖片會有多項 label 時, 也可以使用 Jupyter Notebook 的互動功能來標記 (python 的 multi-label-pigeon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d125d",
   "metadata": {},
   "source": [
    "##### Python: multi-label-pigeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95959bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir flower_images\n",
    "for filename in 100080576_f52e8ee070_n.jpg 10140303196_b88d3d6cec.jpg 10172379554_b296050f82_n.jpg; do\n",
    "  gsutil cp gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/$filename flower_images\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1beba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flower_images/10140303196_b88d3d6cec.jpg', 'flower_images/10172379554_b296050f82_n.jpg', 'flower_images/100080576_f52e8ee070_n.jpg']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56bd073bf3e40d0bbe250c97ed8c64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='0 examples annotated, 4 examples left')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e393be44a3e94f65a515ca709bd30d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='daisy', style=ButtonStyle()), Button(description='tulip', style=ButtonStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95105b61821a498d90a7fcf487b111d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='yellow', style=ButtonStyle()), Button(description='red', style=ButtonStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26a1564062743b5a1a6a3cea3de09b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='indoors', style=ButtonStyle()), Button(description='outdoors', style=Button…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8575f36450b84445b021a98e7fda43dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='done', style=ButtonStyle()), Button(description='back', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2dc2c701d142a7880c52d841ab9712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "filenames = glob.glob('flower_images/*.jpg')\n",
    "print(filenames)\n",
    "\n",
    "from multi_label_pigeon import multi_label_annotate\n",
    "from IPython.display import display, Image\n",
    "\n",
    "annotations = multi_label_annotate(\n",
    "    filenames,\n",
    "    options={'flower':['daisy','tulip', 'rose'], 'color':['yellow','red', 'other'],'location':['indoors','outdoors']},\n",
    "    display_fn=lambda filename: display(Image(filename))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f037ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flower_images/10140303196_b88d3d6cec.jpg': {'flower': ['daisy', 'daisy'], 'color': ['yellow', 'yellow'], 'location': ['indoors', 'outdoors', 'outdoors']}, 'flower_images/10172379554_b296050f82_n.jpg': {'flower': ['tulip'], 'color': ['red'], 'location': ['outdoors']}, 'flower_images/100080576_f52e8ee070_n.jpg': {'flower': ['daisy'], 'color': ['yellow'], 'location': ['outdoors', 'indoors']}}\n"
     ]
    }
   ],
   "source": [
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53672f7",
   "metadata": {},
   "source": [
    "### 自動標記標籤\n",
    "* `Noisy Student`: \n",
    "    * 先人工手動標記一部份影像, 再利用這些影像訓練出 teacher model 並讓他預測更多的標籤\n",
    "    * 再將兩者的組合當作訓練資料, 並利用 dropout 與 data augmentation 去訓練出 student model\n",
    "    * 把表現得更好的 student model 更新成為新的 teacher, 不斷迭代訓練下去; 人工也可以在這時手動標記那些被模型視為預測信心值不高的影像\n",
    "* `Self-Supervised Learning`:\n",
    "    * 有時候在取得圖片時, 還無法立即得知他的標籤 (例如: 診斷前一段時間拍攝的醫學影像, 下雨或打雷前的氣象圖...)\n",
    "    * 所以, 很多時候拍攝的當下無法標記出的影像, 也可能是值得保留的！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947d0ba",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bias (偏見)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb47e02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
