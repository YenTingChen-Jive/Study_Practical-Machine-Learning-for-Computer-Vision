{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e3bda1",
   "metadata": {},
   "source": [
    "# **Chapter 05. Create Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095a01f",
   "metadata": {},
   "source": [
    "* 基本上, 任何能夠以 `4D tensor (batch × height × width × channel)` 的形式作為輸入的資料都可以應用於電腦視覺的訓練方式！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b001602",
   "metadata": {},
   "source": [
    "---\n",
    "##### 影像:\n",
    "* 影像解析度 與 運算／儲存／傳輸所需資源 的 trade-off, 建議 `選擇壓縮格式(例如JPEG)、較高閾值(95%+)、較低解析度(取決於任務精細程度)`\n",
    "* 通常影像會有3個channel(RGB), 但有些影像會有4個channel(RGBA), 其中A是 `alpha(透明度)`\n",
    "* 常規影像處理流程: `影像以壓縮字串的形式讀取, 轉換為 3D uint8 tensor, 再將 [0, 255] 的像素值轉換為 [0, 1] 的浮點數`\n",
    "* 現代最常用的排序是 [height, width, channel] 的順序, 稱為 channel-last 表達法, 例如 TensorFlow 就是如此 (早期則是 channel-first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d92e2a",
   "metadata": {},
   "source": [
    "##### 地理空間資料:\n",
    "* 從地圖產生的地理空間資料, 通常有可以被視為頻道的`柵格頻帶(raster band)`, 也就是具有特定特徵的像素(或更大的網格)被標註\n",
    "* 例如: 圖片中的河流所在的網格, 像素值會被設定為1; 或是有15個州的地圖, 會產生15個頻道, 每個頻道各自標出一個州所在的像素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c992e",
   "metadata": {},
   "source": [
    "#### 音訊(audio)與視訊(video):\n",
    "* 音訊是 1D 信號, 視訊則是 3D。 通常會建議採用專為他們設計的 ML 技術, 但其實也能夠利用處理影像的 ML 方法來簡單處理他們\n",
    "* 音訊方面, 可以用 time-domain 或 frequency-domain 的資料當作輸入, 或是也可以用 NLP 的方式來處理它\n",
    "* 視訊方面, 可以用 Conv3D 或是 其與RNN的結合來處理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826081f",
   "metadata": {},
   "source": [
    "---\n",
    "### 手動標記標籤\n",
    "* 可以`將各個類別都建立資料夾`, 再把對應的圖片放到資料夾中; 或是利用單一`CSV檔記錄下所有資料的 URL + label`\n",
    "* 但 當圖片有不只一項 label 時, 就只能使用後者的方法了\n",
    "* 大規模資料需要標記時, 可以使用 Computer Vision Annotation Tool 這個軟體來達成, 他提供了一個很好用於標記的UI介面\n",
    "* 或是當一張圖片會有多項 label 時, 也可以使用 Jupyter Notebook 的互動功能來標記 (python 的 multi-label-pigeon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d125d",
   "metadata": {},
   "source": [
    "##### Python: multi-label-pigeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95959bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir flower_images\n",
    "for filename in 100080576_f52e8ee070_n.jpg 10140303196_b88d3d6cec.jpg 10172379554_b296050f82_n.jpg; do\n",
    "  gsutil cp gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/$filename flower_images\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1beba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flower_images/10140303196_b88d3d6cec.jpg', 'flower_images/10172379554_b296050f82_n.jpg', 'flower_images/100080576_f52e8ee070_n.jpg']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56bd073bf3e40d0bbe250c97ed8c64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='0 examples annotated, 4 examples left')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e393be44a3e94f65a515ca709bd30d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='daisy', style=ButtonStyle()), Button(description='tulip', style=ButtonStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95105b61821a498d90a7fcf487b111d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='yellow', style=ButtonStyle()), Button(description='red', style=ButtonStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26a1564062743b5a1a6a3cea3de09b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='indoors', style=ButtonStyle()), Button(description='outdoors', style=Button…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8575f36450b84445b021a98e7fda43dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='done', style=ButtonStyle()), Button(description='back', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2dc2c701d142a7880c52d841ab9712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "filenames = glob.glob('flower_images/*.jpg')\n",
    "print(filenames)\n",
    "\n",
    "from multi_label_pigeon import multi_label_annotate\n",
    "from IPython.display import display, Image\n",
    "\n",
    "annotations = multi_label_annotate(\n",
    "    filenames,\n",
    "    options={'flower':['daisy','tulip', 'rose'], 'color':['yellow','red', 'other'],'location':['indoors','outdoors']},\n",
    "    display_fn=lambda filename: display(Image(filename))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f037ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flower_images/10140303196_b88d3d6cec.jpg': {'flower': ['daisy', 'daisy'], 'color': ['yellow', 'yellow'], 'location': ['indoors', 'outdoors', 'outdoors']}, 'flower_images/10172379554_b296050f82_n.jpg': {'flower': ['tulip'], 'color': ['red'], 'location': ['outdoors']}, 'flower_images/100080576_f52e8ee070_n.jpg': {'flower': ['daisy'], 'color': ['yellow'], 'location': ['outdoors', 'indoors']}}\n"
     ]
    }
   ],
   "source": [
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53672f7",
   "metadata": {},
   "source": [
    "### 自動標記標籤\n",
    "* `Noisy Student`: \n",
    "    * 先人工手動標記一部份影像, 再利用這些影像訓練出 teacher model 並讓他預測更多的標籤\n",
    "    * 再將兩者的組合當作訓練資料, 並利用 dropout 與 data augmentation 去訓練出 student model\n",
    "    * 把表現得更好的 student model 更新成為新的 teacher, 不斷迭代訓練下去; 人工也可以在這時手動標記那些被模型視為預測信心值不高的影像\n",
    "* `Self-Supervised Learning`:\n",
    "    * 有時候在取得圖片時, 還無法立即得知他的標籤 (例如: 診斷前一段時間拍攝的醫學影像, 下雨或打雷前的氣象圖...)\n",
    "    * 所以, 很多時候拍攝的當下無法標記出的影像, 也可能是值得保留的！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947d0ba",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Bias (偏見)\n",
    "* 一個 `biased` 的資料集, 代表 `某些範例在資料集中的代表性不足或過多`, 以至於在遇到這些場景時準確率下降\n",
    "* bias 的三個來源:\n",
    "    1. `Selection Bias`: 訓練的資料是這個場景的偏斜子集合(skewed), 原因可能為:\n",
    "        * 某些類型的資料取得比其他類型容易很多, 例如法國/義大利畫家畫像 vs. 斐濟/牙買加畫家畫像\n",
    "        * 訓練資料是在較短的時間範圍內取得的, 例如晴天 vs 各種天候\n",
    "        * 不恰當的資料清理, 例如貝殼資料集中拋棄出現動物的照片, 會導致甲殼類辨識的結果不佳\n",
    "    2. `Measurement Bias`: 蒐集影像的方式在訓練與測試資料中不同, 例如:\n",
    "        * 訓練資料以較高畫質的相機拍攝, 而測試資料來自畫質較低的攝影機\n",
    "        * 狗與狼的分類器中, 如果狗的照片大多都在草地上拍攝, 而狼的照片大多來自於雪地, 模型可能學習到的是草和雪的分類特徵\n",
    "    3. `Confirmation Bias`: 現實生活中值的分佈會導致模型強化不想要的部分, 例如:\n",
    "        * 現實生活中, 消防員通常為女性, 這可能導致女性消防員的圖片被認為是cosplay的假消防員\n",
    "        * 新聞過度強調某些族群的犯罪記錄, 會導致族群與犯罪有不預期的掛鉤\n",
    "* biased $\\neq$ imbalanced ! 不平衡可以是正常的現實資料分佈, 只要他不會讓模型以不想要的方式表現資料集的任何層面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb47e02",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **建立資料集**\n",
    "* 常見的 training/validation/testing 比例可以是 80:10:10, 或是只需要 training/validation 時 80:20\n",
    "* 或是在需要 `交叉驗證(cross-validation)`時, 將 10% 固定為 testing, 其餘隨機分割為 training/validation 來多次訓練模型 -> 常用於小型資料集的訓練\n",
    "* 注意: 不要在每一次訓練前重新拆分這三組資料, 否則會讓準確率無法被拿來比較"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f853e54",
   "metadata": {},
   "source": [
    "#### TensorFlow Record (TFRecord)\n",
    "* TFRecord 是推薦的資料格式, 它是有 image & label 兩個 key 的字典, 也可以嵌入更多的 metadata (例如 bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99884282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 3670; Training: 2930, Validation: 359, Testing: 381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/all_data.csv', names=['image','label'])\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "rnd = np.random.rand(len(df))\n",
    "train = df[ rnd < 0.8  ]\n",
    "valid = df[ (rnd >= 0.8) & (rnd < 0.9) ]\n",
    "test  = df[ rnd >= 0.9 ]\n",
    "print(f\"Total images: {len(df)}; Training: {len(train)}, Validation: {len(valid)}, Testing: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a94b0e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/10466290366_cc72e33532.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/11642632_1e7627a2cc.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/13583238844_573df2de8e_m.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/1374193928_a52320eafa.jpg',\n",
       "        'daisy']], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_csv('TFRecord/train.csv', header=False, index=False)\n",
    "valid.to_csv('TFRecord/valid.csv', header=False, index=False)\n",
    "test.to_csv('TFRecord/test.csv', header=False, index=False)\n",
    "\n",
    "outdf = test.head()     ## len(outdf) == 5\n",
    "outdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Labels: 5 , including daisy, dandelion, roses, sunflowers, and tulips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/10466290366_cc72e33532.jpg\n",
      "gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg\n",
      "gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/11642632_1e7627a2cc.jpg\n",
      "gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/13583238844_573df2de8e_m.jpg\n",
      "gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/1374193928_a52320eafa.jpg\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.io.gfile.GFile('gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dict.txt', 'r') as f:\n",
    "    LABELS = [line.rstrip() for line in f]\n",
    "print('Total Labels: {} , including {}, {}, {}, {}, and {}'.format(\n",
    "    len(LABELS), LABELS[0], LABELS[1], LABELS[2], LABELS[3], LABELS[4]))\n",
    "\n",
    "## --------   把資料轉成 TFRecord 的格式   --------\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "\n",
    "def _string_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def read_and_decode(filename):\n",
    "    IMG_CHANNELS = 3\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def create_tfrecord(filename, label, label_int):\n",
    "    print(filename)\n",
    "    img = read_and_decode(filename)\n",
    "    dims = img.shape\n",
    "    img = tf.reshape(img, [-1]) # flatten to 1D array\n",
    "    return tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _float_feature(img),\n",
    "        'shape': _int64_feature([dims[0], dims[1], dims[2]]),\n",
    "        'label': _string_feature(label),\n",
    "        'label_int': _int64_feature([label_int])\n",
    "    })).SerializeToString()\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    (p \n",
    "     | 'input_df' >> beam.Create(outdf.values)\n",
    "     | 'create_tfrecord' >> beam.Map(lambda x: create_tfrecord(x[0], x[1], LABELS.index(x[1])))\n",
    "     | 'write' >> beam.io.tfrecordio.WriteToTFRecord('TFRecord/output/train')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7646de72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardcoded:  train a train True\n",
      "hardcoded:  train a valid False\n",
      "hardcoded:  train b train True\n",
      "hardcoded:  train b valid False\n",
      "hardcoded:  valid c train False\n",
      "hardcoded:  valid c valid True\n",
      "hardcoded:  valid d train False\n",
      "hardcoded:  valid d valid True\n"
     ]
    }
   ],
   "source": [
    "## splitting in Apache Beam\n",
    "def hardcoded(x, desired_split):\n",
    "    split, rec = x\n",
    "    print('hardcoded: ', split, rec, desired_split, split == desired_split)\n",
    "    if split == desired_split:\n",
    "        yield rec\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        splits = (p\n",
    "                  | 'input_df' >> beam.Create([\n",
    "                      ('train', 'a'),\n",
    "                      ('train', 'b'),\n",
    "                      ('valid', 'c'),\n",
    "                      ('valid', 'd')\n",
    "                  ]))\n",
    "        \n",
    "        split = 'train'\n",
    "        _ = (splits\n",
    "                 | 'h_only_{}'.format(split) >> beam.FlatMap(\n",
    "                     lambda x: hardcoded(x, 'train'))\n",
    "         )        \n",
    "        split = 'valid'\n",
    "        _ = (splits\n",
    "                 | 'h_only_{}'.format(split) >> beam.FlatMap(\n",
    "                     lambda x: hardcoded(x, 'valid'))\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
